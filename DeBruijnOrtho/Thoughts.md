Мысли

Параметры теста, который рассматривает Сергей Трошков. Это 500 тыс. цепочек, из которых получается 40 млн. узлов. Не так много, но и немало. Это до слияния. Произведем слияние, создадим таблицу имен.  Кроме множества узлов, создадим множество дуг. Получится множество пар { узел, узел }. Это множество индексируем по первому узлу и индексируем по второму узлу. Всего дуг - (почти) ровно 50 млн. По первому индексу будем искать узлы с кратностью...

Сейчас пытаюсь реализовать схему нахождения транзитных цепочек с кратными дугами. Но вначале надо бы разобраться с тем как вообще могла получиться цепочка длиной 159 - это почти удвоенная длина рида. Действительно, есть два рида, которые стыкуются на этой цепочке. Но как, за счет какого сочетания факторов? Решение видимо лежить на границах каких-то участков. Пусь

Измеряю. В тесте reads.txt
495108 lines
39_838_673 word
23_500_652 различных слов
21008101 транзитных узлов
335935 число цепочек
159 максимальная длина цепочки

Затраченное время: 21.4 сек.   

цепочка длиной 159 символов
TTAAACTAGTATTTAATCCCTTCCTGGACCTCAGCATCCTTACCTATAACCTAAGGATACTCATTAATAAATAAGGTCAGTTTCTGACTCCGTCCTTTGGTAGTTCTACTGTTTTGAATTTAAATTCAAACTCCCTAAGTGAAAGTGCAATGATACAAAAGTGAAAGGAAAGCAACTG
найдена стр. № 7900
CTTAAACTAGTATTTAATCCCTTCCTGGACCTCAGCATCCTTACCTATAACCTAAGGATACTCATTAATAAATAAGGTCAGTTTCTGACTCCGTCCTTTGG
стр. № 58474
GTTTCTGACTCCGTCCTTTGGTAGTTCTACTGTTTTGAATTTAAATTCAAACTCCCTAAGTGAAAGTGCAATGATACAAAAGTGAAAGGAAAGCAACTGTG


Надо вернуться к основам. Первая основа - построение графа. Узлы строятся правильно. Строим дуги. Для узла могут быть входящие дуги и могут быть исходящие. Сейчас мы строим только единичные входящие и исходящие дуги. При построении множества узлов, мы имеем состояние, заключающееся в наличии узла current и возможного наличия узла previous. Дуги проводятся ТОЛЬКО если есть previous. Для узла current, мы пытаемся построить ссылку prev, для предыдущего узла, мы пытаемся построить ссылку next. Пытаемся следущим образом: если ссылка null, то строим, если ссылка не null, но такая же, как мы строим, то оставляем ссылку, если ссылка есть, то другая, то делаем ссылку на dummy. 

Посмотрел, вроде граф формируется правильно. Теперь как его использовать? Надо обрабатывать узлы. От узла надо пойти к началу цепочку. Идти надо по обратным (prev) ссылкам. Начальный узел - это узел, у которого есть ровно одна ссылка на следующий (next). После этого, заводится список для цепочки, первый узел отмечается как обработанный и помещается в список. Далее, двигаемся по ссылкам next. Следующий узел отмечается как обработанный и помещается в список.  

### 20190831 07:41
Последний день лета... Как-то бестолково пролетело и я в этом не сильно виноват...

Пытаюсь избавиться от "бяки" в реализации алгоритма выделения цепей. Похоже, происходит зацикливание. Но выявить пока не получается. Альтернативный алгоритм мог бы быть построен на сканировании узлов и выделении начальных узлов цепочек. По некоторым признакам. А потом уже выделением всей цепочки через движение вперед. Вопрос: а всегда ли есть в цепочке признак начала? Теоретически, может и не быть. Цепочка при этом "отпадет" от графа, но такое может случиться, кажется... Этот же момент может быть и в моей реализации движения назад. Попробую это выявить.  

Выявил, гипотеза подтверждается. Я подумал и решил, что цикл можно разорвать тем, что проверять на попадание на текущий элемент при "движении назад".

Вроде получилось, зафиксирую последний результат. 
```
Hello de Brein!
lines:495108 words: 40333781 different: 23770815
Total 23166991 singles
Build nodes ok. Duration=22129
361676 chains
Build chains ok. Duration=1326
nchains=361676  maxchain=894
TGTCGCTGACCTATCCTGTAATCATTCTATAAGAAAATCCATTCCAAGAATATTTACTTAGATGTTATAATATTTATTATAACAAATTAAGGGATATTTATAAATTTGGGTATATCTCATAAGATACATATAAATATTAATATAATAATGCTGATTAATCTTAATATAACTACCATAAAGATATAATTTAACGCTTTTAATTTTTTTATCCACCGAATCCTTTACGACCATTATTAACTCATTCGTCATATTTTCTGTAATAAAGTAAGTTCCCCCAGGAACATTTTCTAGCAAACGATATATTAGACAACAATCTTCTGGAGGATGATAAAAATTTAGCTTTTTTAAATTCTTCCGTAATTTTTTTTCCTGGATTTTACGAGACCTACGATAAAATATACATTTTATCGGATTGCGAAATAACCTAACCATCTTCCTCGACTAACTAAGGATAGTTTCCTTATTTTAACAGTCATTAGAAATAAAGTTTATTACAAAATATATTAATTTTTTATAAAATTATTTCTTGTTAAAAATCATCATTTTTAATTGGAATCCTATTATAGACAATAAAATATCATTATTAGTAATAATTAAACGGCTCTAAATTTTAGTTCTTTGTCTTCAACTTAGAGGTCTAGTATAGGGTAGCCACTATGAAGGTGCTAATTCTAGTACTACTCGGGGTGGTTATCCTTCAGGCCGCACCTATACGTAAATTAGAAGATCTACTACCAACCCGTTATCCTCCTAAACATGAGCTGGTTTATTGGTGCACCTACGCAAATCAATGTGACTTTTGCTGGGAATGCGTCCACGGTATCTGCCGAAACAGGATTCAAGCAGATTGGCCAGTTATTCACCAAAATGACTGGATTATAAATTGCACGGTTTCCCGCTGGCTAAATAAATG
duration=1326
```
Максимальный захват памяти 4170 Мб. 

А как решать эту задачу по-другому? С помощью кластера. Есть 23 млн. узлов, в узле есть информация, напр. 10 байтов и две ссылки по 8 байтов, еще признак просмотренности. Итого, 27 байтов. Всего - меньше гигабайта. Приближаемся к человеческому геному. Но все же, с идеями напряженка.

Часть распределения по процессорам/задачам - отдельная работа с узлами и с графом. Действительно, содержимое узлов (код), при анализе графа нам не существенно. Если ссылки реализовывать целыми, то будет 2 целых и одно целое - ссылка на узел. 12 байтов. Всего 250 Мб. ОЗУ. Уже можно всю "человеческую" информацию загнать в один компьютер с ОЗУ в 50 Гб. Это также палеотив, надо все же распараллеливать. 

Рассмотрим множество ребер { исток, инфо, сток }. Я могу это индексировать. Тогда индекс Sources и Targets дадут возможность по заданному имени узла получать все истоковые или стоковые дуги.  

Попробую проанализировать задачу. Пусть огромные (больше объема ОЗУ) массивы данных накапливаются на мастере. Первый массив - множество пар { номер, код }. Чтобы можно было бы процесс распараллелить, номера не обязательно последовательные. В процессе сканирования ридов, берем очередной код, выполняем шардеринг и преобразуем его в номер. Пару записываем в последовательность. Сейчас обрабатывается 40 млн. слов за 20 сек. , т.е. 2 млн. запроса в сек. Теоретически, TCP дает возможность выполнять до 100 тыс. запросов в сек.

Итак, создание множества узлов и первичной системы дуг, потребуется работа распределенной таблицы имен в режиме одиночных запросов. Таблицу дуг надо индексировать. Индексирование можно сделать через массовую операцию и через кластер. Пусть есть множестов пар { source, target }, отправляем множество пар { source, na } на сортировку, получаем множество исключаемых пар, можем лишние дуги исключить простым сканированием, если это множество будет упорядочено по номеру пары (дуги). Далее, нужно движение по дугам. Соответственно, нужны распределенные индексы и динамика единичных запросов.

### 20190914 09:28
Начинаю проект DeBreinData. Это будет решение, соответствующее тому, которое есть в проекте DeBrein, но с ориентиацией на большие данные и кластер.

Первая часть работы построение графа. Сканирование файла ридов будет таким же, другим будет формируемый граф. Формируемый граф будет имитироваться key-value хранилищем, ссылки будут вперед и назад в виде кода узла. Кодом узла будет номер узла.  

Только узел придется разделить на части - часть это строковый код, а часть - ссылки и другие поля. Есть впечатление, что строковый код нужен редко и что хеш-таблица по нему нужна только в начале. Буду создавать эту таблицу как последовательность кодов. Пусть таблица будет [sstring], индекс по полю будет ...

Сколько будет "весить" первая хеш-таблица? Есть 500 тыс. ридов, каждый преобразуется в в порядка 100 узлов. Часть узлом может совпадать. В нашем случае, получилось порядка 24 млн. входов. Как обойтись без хеш-словаря? Можно отсортировать, но это будет не достаточно быстро. К тому же, досортировка будет делаться медленно. Хотя это путь... Другой вариант - распределенный словарь. Проблема - в сетевой "медленности" одиночных запросов. Нужны массовые запросы. В принципе, возможен дополнительный прогон. Сначала синтаксически разбрасываем по модулям, модули сортируют, формируя свои словари. Потом будем массово запрашивать и получать коды. Под этими кодами, мы сохраняем формируемые узлы. При появлении информациии, дописываем информацию в узлы. Как-то так...

Получается, что надо каждый модуль "снабдить" своим диапазоном кодов, при первом проходе, снабдит слова кодами и устранит дублирование. Распределенная система словарей будет выдавать по слову код и по коду слово. Собственно построение словаря будет производиться следующим образом: 
1. Есть анализируемая последовательность рида, мы находимся в   

### 20190915 07:27
Пока досыпал или просыпался, придумал модифицированное решение. Суть заключается в том, чтобы синтаксические свойства генерируемого идентификатора (кода) для узла, соответствовали синтаксическим свойствам слова. Это не только увеличит эффективность обработки, но и упростит логику всегно построения. Насчет эффективности обработки, возможно, я не прав. Была идея того, чтобы коды одного рида попадали в одну группу. Но при новом подходе это не получится. 

Попробую изложить логику обработки, соответствующую новому подходу. Теперь не надо делить сканирование массива ридов на проходы. Имеем предыдущий узел (если есть), имеем очередное слово, запрашиваем хранилище, получаем текуший узел. Далее, производим модификации узлов, предыдущий, если есть и если модифицирован, сохраняем, текущий делаем предыдущим или сохраняем, если он последний в риде и изменен. Процедура вполне понятна, но как насчет массового исполнения? Массовое исполнение получится за счет обработки нескольких ридов одновременно. Как это удобнее сделать?

Заводим объект рид и итеративно подвергаем его обработке по предложенной схеме. Множество обрабатываемых ридов будем пополнять по мере выхода ридов из обработки. 

А еще я подумал над синхронизацией. В предложенном варианте, когда "путешествует" запись объекта, синхронизацию будет осуществлять трудно. Несколько более сложная схема, позволит это делать надежно. Имеется ввиду посылка более определенной команды типа "в записи с кодом C значением V изменить поле pred". 

### 20190916 07:56
Если не сильно оптимизировать, то единичные команды к хранилищу будут:
```
int GetNodeId(string word);
void SetNodePrev(int node, int prevlink);
void SetNodeNext(int node, int nextlink);
```

### 20190918 10:01
Прошлым достижением было то, что удалось построить граф уже в идее key-value. Только пока без "распараллеливания". Почему-то обработка несколько замедлилась. Надо посмотреть почему.

При старом подходе, построение графа выполнялось 18 сек. и выделение цепочек выполнялось 1.3-1.4 сек. При новом подхоходе граф стрится 19-20 сек. И оперативной памяти в обех свариантах захватывалось около 4 Гб. Даже не буду заморачиваться с анализом... Теперь косвенно проверим результат. Проверить до конца не удалось, но пока проверял, исправил две ошибки, надеюсь, теперь будет все нормально. 

Последние параметры графа: число узлов 23770815, число ссылок на предудущий 23464396, число ссылок на следующий 23462285.

Буду трансформировать программу, а потом дописывать. Трансформация программы заключается в разбиении метода Process() на мелкие действия. Причем мелкие действия разных ветвей Process не мешают друг другу. Потом эи действия будем собирать в группы независимых действий и выполнять эти группы. 

Сделал приготовления в коде: убрал цикл в Process() и размельчил действия. Каждое обращение к хранилищу делается в конце обработки, следующие состояния прямо указываются. Теперь надо куда-то получить команду работы с хранилищем, эта команда должна быть снабжена также посылающим объектом Reed, в объекте найдется следующее состояние. Цикл обработки будет следующий: заглушка для команды выполняется, новое состояние устанавливается, а в исполнительнуя часть "уходит" объект Reed и команда для хранилища. 

### 20190919 08:45
Предположим, мы сделали буфер исполнения ридов. Буфер пополняется новыми ридами и освобождается от уже обработанных. Буфер работает "ортогонально" основному процессу. То есть, шаг обработки заключается в том, что исполняется шаг во всех (!) ридах и формируется "широкая" команда хранилищу. Посылаем команду, получаем "широкий" ответ. Элементы этого ответа в точности соответствуют буферу. Исполняем лямда-функции на этих значениях. Как-то так... Видно, что схема может быть более асинхронной и, может быть, более эффективной, но пока надо стремиться к простоте. 

Применил BufferredProcessing, результаты получились, но несколько отличные. В частности, число получившихся ссылок вперед и назад получилось где-то на 300 тыс. меньше в каждом направлении (269966 и 270115). 

Прогон графа reads7x3.txt дает: 30 слов, разных 14, 5 цепочек.

### 20190920 07:14
Почему получилось расхождение, я не выявил, проверил на малых данных, вроде соответствует...

Но теперь я придумал простую схему для реализации "горизонтальных" запросов к хранилищу. Суть идеи в том, что в каждом элементе Reed завести три поля: команда, аргументы и результат. Соотвественно, в рамках одного шага процесс заполняет два поля. Потом выполянется массовое обращение к хранилищу, потом процесс как-то фиксирует результат. Это можно сделать в виде дополнительного состояния и метода (Fix?). Но похоже, фиксировать лучше с помощью лямбда функции, точнее - Action. Попробую. 

Попробовал, сгруппировал горизонтальную обработку, пока работает. Теперь надо сделать удаленный запрос или его имитацию. TCP запрос будем делать по правилам Поляра. Это значит, что первые байты (8) - количество запросов, дальше идут запросы. Запрос имеет следующий тип:
```
Comm = Empty^None,
    GetNodeId^{ word: sstring },
    SetNodeNext^{ node: integer, nextlink: integer },
    SetNodePrev^{ node: integer, prevlink: integer };
```
Проблема описания в том, что тип результата надо указывать дополнительно. 

### 20190921 07:01
Теперь сделаю преобразование набора команд в поток (массив) байтов. Потом исполню этот массив, потом верну результат и преобразую назад. 

Кажется, технологический прототип я сделал. Без реальной передачи по сети, но с выделением всех этапов. Что получается? Для группирования обработки ридов по 1 тыс., мы получаем число обменов около 200 тыс. посылаемыми массивами в 22 тыс. байтов. Время полной обработки без дополнительного времени на коммуникации 54 сек. Что реально это означает пока не очень понятно потому что неизвестна скорость обмена массивами такой длины. Гипотетически, это могло бы быть около 6 сек. Но эти скорости (32 мс. на 1000 обменов порциями в 22 тыс. байтов) вряд ли достижимы в реальном обмене. В общем, надо проверять...

### 20190922 09:46
Программа получилась очень "корявая". И не факт, что правильная... А как писать красиво и эффективно? Анализ показывает, что я пытаюсь написать асинхронную программу синхронными средствами. Я разбиваю общий поток вычислений на шаги, комбинирую эти шаги, я меняю порядок вычислений. Если разбить программу на шаги s1, s2, s3, ..., а потом приметить программу к множеству аргументов, то может статься, что отдельные шаги для разных аргументов можно выполнять независимо или параллельно. Тогда я группирую шаги si(a), si(b), ... и выполняю их одновременно. Причем можно делать это синхронно, можно асинхронно. В частности, пул исполнения может уменьшаться за счет выполненных шагов и пополняться за счет продолжения  
Почитал про асинхронное программирование. Не очень понятно, особенно в контексте решаемой задачи. Но хочу попробовать сочинить прозрачный тест. Идея заключается в "проходе" прямоугольной матрицы. Пусть есть два числа: номер серии и номер шага. Программа запускается с каким-то номером серии и все значения этого потока будут иметь этот номер серии, но различные номера шагов. Можно также работать с элементами матрицы, где используются номер рядка и номер колонки. Для наглядности, значение шага будет вычислятья как ir * 1_000_000 + ic. ic меняется последовательно в рамках некоторого процесса.   

### 20191022 13:52
Выявил некоторую особенность: в поле next осли оно не отрицательное, в 280 раз чаще встречается следующий номер, чем какой-то другой. В принципе, это объяснимо, но как этим воспользоваться?

### 20191023 05:50
Более или менее воспроизвел предыдущие результаты, вот распечатка:
```
Start DeBreinOrtho!
lines:0 words: 0 nodes: 23770815
Build nodes ok. Duration=20992
Build statistics ok. Duration=297
11135 297395 295294 23166991
follow: 23379408 not follow: 82877
==== nchains: 361676  maxchain: 894
TGTCGCTGACCTATCCTGTAATCATTCTATAAGAAAATCCATTCCAAGAATATTTACTTAGATGTTATAATATTTATTAT
AACAAATTAAGGGATATTTATAAATTTGGGTATATCTCATAAGATACATATAAATATTAATATAATAATGCTGATTAATC
TTAATATAACTACCATAAAGATATAATTTAACGCTTTTAATTTTTTTATCCACCGAATCCTTTACGACCATTATTAACTC
ATTCGTCATATTTTCTGTAATAAAGTAAGTTCCCCCAGGAACATTTTCTAGCAAACGATATATTAGACAACAATCTTCTG
GAGGATGATAAAAATTTAGCTTTTTTAAATTCTTCCGTAATTTTTTTTCCTGGATTTTACGAGACCTACGATAAAATATA
CATTTTATCGGATTGCGAAATAACCTAACCATCTTCCTCGACTAACTAAGGATAGTTTCCTTATTTTAACAGTCATTAGA
AATAAAGTTTATTACAAAATATATTAATTTTTTATAAAATTATTTCTTGTTAAAAATCATCATTTTTAATTGGAATCCTA
TTATAGACAATAAAATATCATTATTAGTAATAATTAAACGGCTCTAAATTTTAGTTCTTTGTCTTCAACTTAGAGGTCTA
GTATAGGGTAGCCACTATGAAGGTGCTAATTCTAGTACTACTCGGGGTGGTTATCCTTCAGGCCGCACCTATACGTAAAT
TAGAAGATCTACTACCAACCCGTTATCCTCCTAAACATGAGCTGGTTTATTGGTGCACCTACGCAAATCAATGTGACTTT
TGCTGGGAATGCGTCCACGGTATCTGCCGAAACAGGATTCAAGCAGATTGGCCAGTTATTCACCAAAATGACTGGATTAT
AAATTGCACGGTTTCCCGCTGGCTAAATAAATG
Extract chains ok. Duration=861

Максимальный расход памяти: 3700 Мб.
```
Теперь пора двигаться к большим данным. Желательно сначала обеспечить жесткую преемственность, а уже потом подумать над 
оптимизациями. 

Делаю StorageServer и StorageClient. Сначала в виде простых классов и организации выполнения заданий по построению графа и 
извлечению цепочек. Главное, делать модификацию шаг за шагом постоянно контролируя корректность преобразования. Начинаю, естественно
с клиента, он будет делать все пока не уберу функции в сервер.

Фиксирую сокращенный тест (10 тыс ридов):
```
Start DeBreinOrtho!
lines:10000 words: 0 nodes: 712541
Build nodes ok. Duration=421
Build statistics ok. Duration=7
137 8470 8480 695454
follow: 701505 not follow: 2429
==== nchains: 9773  maxchain: 452
TAAACTCTAAGATATGGCAGAATTTAATATTGATGAGCTTCTCAAAAACGTATTGGAGGATCCCTCTACTGAAATATCCGAAGAAACGCTTAAACAGCTTTACCAAAGGACGAACCCTTACAAACAGTTCAAAAATGATAGCAGGGTGGCCTTTTGCTCTTTTACAAATTTGCGGGAGCAGTATATTCGACGTCTTATAATGACTAGCTTTATTGGATATGTCTTCAAAGCTCTGCAGGAATGGATGCCTTCCTATTCAAAACCTACCCACACGACCAAAACTCTTCTCAGTGAGCTAATAACGTTAGTTGATACTTTGAAACAGGAAACTAATGATGTTCCCTCTGAATCGGTAGTAAATACAATTTTATCTATAGCGGATAGCTGCAAAACCCAGACGCAGAAAAGCAAGGAAGCTAAAACAACGATCGATAGCTTTTTACGAGAACATTTTGTGTTTGATCCTAATCT
Extract chains ok. Duration=41
```

К сожалению, время расчета при модификации потихонтку растет и уже на полных данных составляет 34 сек. По ходу дела, я избавился в 
от двух состояний, видимо про эти состояния Сергей Трошков говорил, что там одни нули. Пора делать сервер. В клиенте напрямую 
реализуются нужные методы, а надо их реализовывать отдельно. Сначала можно сделать это через класс StorageServer и его прямое 
использование в StorageClient. Вроде это ничему не мешает, а несколько приближает нас к искомому решению. 

### 20191024 06:51
С утра долго "разогревался" с помощью пасьянса и шахмат, теперь уже работаю. Первое достижение: я придумал как представлять 
"ортогонализацию". В ReedClient сделал статический метод обработки потока ридов. У метода сделал простой (очевидный) вариант 
реализации - последовательный перебор ридов. А в программе построения графа просто использую буферизированное исполнение. 
Все получилось и даже (почему-то) стало быстрее работать - построение теперь снова 18-19 сек. Сейчас это снова проверю. 
Странно, но это действительно так...

Теперь буду делать "ортогональный" способ. Сделал без внешних запросов. Время увеличилось более, чем в 2 раза. Теперь оно 
составляет 40 сек. Такое увеличение мне непонятно - каких-то новых вычислений не прибавилось, может преобразование в массив 
как-то плохо влияет? Эксперименты показали, что базовый вариант вычисления графа действительно работает 19 сек., вариант с шагами
и состояниями работает 33 сек., а "ортогональный" вариант 40 сек. Пока исследований достаточно, надо двигаться в сторону 
сетевого решения.

Теперь перевел формирование графа на универсальный метод SendAction, аргументами которого являются команда, массив объектов 
фактических значений параметров, функция, преобразующая вернувшися результат в что-то для этой веточнки. Все по-прежнему работает.

Чем будет отличаться сетевое решение от модельного? Разрывом между выполнением шага действий и последними действиями. В этом разрыве
- сетевой запрос. Итак, клиент исполняет первую фазу шага, готовит команду и аргументы к ней. Потом команды разных Reed-объектов
группируются и посылаются серверу, сервер отрабатывает команды и возвращает результат, результат отрабатывается сохраненной в объекте 
функцией. Как-то так... Пора продолжить преобразование программы. 

Технологическую модель я сделал. Работает она 60 сек. на полных данных. Результат похож на полученные ранее. 

### 20191025 08:50
60 секунд это одна минута. И еще не факт, что расходы на передачу данных будут небольшие... Надо посмотреть сколько байтов передается
в полной задаче. 

Пропустил задачу, добавив подсчет байтов. Послано 1600 млн. байтов, получено 360 млн. байтов. Кстати, время построения графа
уменьшилось до 52 сек., наверное это из-за перезагрузки компьютера... Даже 51 при следующем прогоне получился...

### 20191026 17:16
Теперь думаю в каком направлении двигаться. Теоретически, код должен бы писать аспирант, но пока у него не получается. Можно
попробовать научить его писать код, вроде парень умный. Можно в принципе дописать код и условно промерить производительность. 
Есть еще одна задача: нахождение наиболее экономных алгоритмов для ключевых частей программы. Ну например, таблица имен нужна 
обязательно. Но необязательно чтобы это была хеш-таблица Dictionary. Надо экономнее расходовать память, а то половоина миллиона 
ридов - уже предел. Если сформировать более экономное решение, то предел может быть на другом объемном уровне. Действительно, если 
это будем простая таблица "строка фиксированного размера" - целое, то строку можно представлять 2x20/8 - 5-6 байтами. Допустим, 
номер кода будет 4 или 8 байтный, тогда пара строка-код будет 9-14 байтов. В "стандартном (16 Гб)" ОЗУ можно поддерживать таблицу 
в миллиард строк (входов). Не 20 млн., а миллиард! А кажется, нужно организовать обработку порядка 100 млрд. узлов. Это выглядит 
возможным. 

Интересно посмотреть какие характеристики будут у текущего в Поляре решения по таблице имен. И какое оно текущее? 

Попробовал, понял, что уже изрядно забыл свое решение по таблице имен. Особенность того, что требуется - надо наращивать данные 
одновременно с поддержанием корректного решения. Все варианты "крутятся" вокруг Dictionary. А это решение - затратно по использованию
памяти. Даже если разбить словарь на множество секций, все равно получится большой объем данных. И не получается вычислять секции 
таблицы имен по одной. Можно экономить на использовании ОЗУ. Насколько Dictionary расходует память? 
 
Минимизировал все действия. Создание словаря заняло 14 сек. Потреблялось 2.5 Гб ОЗУ. А если строки заменить на целые? 

Условная замена строк на целые дала использование ОЗУ в 467 Мб. (оценка по диспетчеру задач).

Есть ряд возможностей по оптимизации таблицы имен. Первая, как уже отмечалось, перевести строки в целые коды, благо здесь все
нужное в наличии. Есть фиксированность формата строки и 2-битный алфавит. Может и более битный, но это детали. Для больших данных
время на выборку из таблицы должно соответствовать времени передачи данных. Это дает возможность применять менее быстрые, но 
алгоритмы более экономные в расходовании памяти алгоритмы. Например, Таблица имен может быть устроена как массив векторов слов.
Это уже потенциально дает экономию в расходовании ОЗУ. А еще, можно через это построение согласовать коды и слова. Поясню. Пусть
таблица слов имеет конструкцию массив длиной 2^k векторов слов произвольной длины. То есть, слова - фиксированной длины, а 
вектора - произвольной. Тогда код будет построен как конкатенация битов номера строки массива и номера в строке. Первая часть кода
будет вычисляться по слову, вторая - по получившейся позиции. В первой части может быть заложен и номер секции в расщеплении таблицы.
Хорошо бы проверить этот подход. Попробую сделать это.

Наилучшие характеристики такого подхода: ОЗУ 400 Мб., время 27 сек. (!).

### 20191028 08:12
Вчера проводил эксперименты на предмет организации таблицы имен. В общем, новых возможностей не выявил.

### 20191029 07:53
Сегодня начал день с того, что провел ревизию программы извлечения цепочек. Дело в том, что вчерашний разговор с Сергеем 
Трошковым навел меня на мысль, что первые узлы цепочек можно определять по критериям и это не требует "движения назад". 
Сделал это изменение, все заработало и выделяет ту же самую цепочку, которую программа выделяла и раньше. Я обошелся без признака 
node.waschecked, но с большим количеством запросов к хранилищу. Для определения является ли очередной узел первым, требуется до 2-х
обращений к хранилищу. Для прохождения цепочки, требуется по 1-му обращению на элемент цепочки. Чтобы уменьшить колпчество обращений 
к хранилишу на стадии проверки на "первость", можно втянуть какую-то информацию о внешности в узел и использовать вместо обращения
к предыдущему узлу. Будет при этом выигрыш или нет - непонятно, пока можно этим не заниматься. 

Характеристики расчета в настоящее время: создание графа 55 сек., выявление цепочек: 2.5 сек. 

Следующее действие - кодирование слов. Начнем с простого кодирования:
```
A - 0
C - 1
G - 2
T - 3
```
И пока будем упаковывать пары бит в двойные слова. Это дает ограничение по длине слова в 32 символа. Потом можно будет пробовать 
кодировать в массив байтов. Но это потом...

Заработала система с кодированными словами. Граф собирается за 42 сек., цепочки выявляются за 1.7 сек. Вопрос вызывает лишь 
использование ОЗУ, которое оценивается на уровне 1 Гб. Я ожидал лучшего... Проведу-ка я эксперимент в тестовой зоне.

"Стандартный" результат - формирование словаря с целым ключом. Время формирования: 9.3 сек., ОЗУ: 400-500 Мб. Использование UInt64
дал 9.8 сек. и 900 Мб. ОЗУ, логично... Даже добавив построение списка кодов имен, получилось 950 Мб. Тогда откуда в основном решении
еще более гигабайта? Попробую найти.

### 20191030 06:31
Подогнал структуру данных в экперименте с таблицей имен и получил приблизительно те же результаты по использованию ОЗУ, т.е. 
около 2 Гб. Вопрос закрыт. 

Что теперь? Надо сделать обращение к хранилищу максимально прозрачно и так, чтобы можно было управлять местом размещения данных.

### 20191031 15:00
Немножко облагородил код, теперь становится понятно как внедрять секции и смешанное исполнение. Смешанным исполнением я называю 
исполнение, когда в качестве секций используются и удаленные компьютеры (по сети) и свой компьютер (через API). Управлять этим можно 
через класс хранилища, который может быть и "местный" и "remote", т.е. используемому через сеть.

Сейчас прочитал письмо от Сергея Трошкова, оно несколько огорчило. Про самую известную программу Spades разработчики пишут, что она 
обрабатывает 28 млн. ридов за 42 минуты. Программе требуется 8.4 Гб. ОЗУ и 23.9 Гб. диска. По косвенным признакам, для человека 
данных будет в 20 раз больше. (около 300 млн. ридов - Сергей).

### 20191102 08:56
Вот и ноябрь наступил... 

Продолжу написание структурированной программы. 

### 20191103 06:56
Ничего я вчера не продолжил, весь день промаялся с головной болью. Сделаю очередную попытку. 

Сначала сделаю два варианта StorageClient: SelfStorage и NetStorage. Парвый предоставляет услуги Storage (StorageClient) в 
виде своего ресурса (объекта), на что будет тратиться оперативная память. А второй есть сетевой клиент. Сетевой клиент запускается
на других машинах. Потом отлажу программу с SelfStorage, потом с NetStorage, потом совместно. Начинаю. 

Какие особенности хранилища должны быть в данной задаче? Есть встроенная таблица имен, но она нужна только на этапе построения 
графа. Вторая особенность в том, что узды пронумерованы сплошной нумерацией. Третья особенность в том, что предполагается расслоение
базы данных хранилища по слову для таблицы имен и по коду для списка узлов. Программа запускается или в режиме мастера или в
режиме слэйва или сетевого хранилища. (появляются знакомые контуры сетевой программы)

Кстати, какая текущая оценка ресурсоемкости моей программы? На 0.5 млн. ридов, это 2 Гб. ОЗУ и 40-50 сек. На задачу в 50 раз больше
будет соответственно 100 Гб. и 50 мин. ОЗУ требуется гораздо больше, чем в программе Spades, но там предположительно, граф
сильно уменьшен. 

Начал формировать конфигурацию мастер-слэйв и обратил внимание, что ранее, в проекте Polar.Datanode у меня была ситуация
"вывернута наизнанку". То есть, мастер был сервером (сервисом), а слэйвы - клиентами. Клиенту при запуске передается сетевой адрес
сервиса (он един), по этому адресу клиент делает запрос и устанавливает связь. А после этого, ждет команды. Возможно, это есть 
нормальная конфигурация взаимодействующих узлов. Но вроде есть и обратная ситуация. Скажем такая: мы запускаем сколько надо 
слэйвов, они запускают свои "слушатели" и ждут мастера. Мастер имеет все сетевые адреса узлов, он отдельным проходом устанавливает 
связь с каждым из слэйвов, а потом начинает загружать их командами. Не знаю в чем разница, попробую старую, работавшую схему.

Итак, как запускается программа? Ей дается обязательный параметр в виде места длля хранения файлов. А для слэйвов еще дается ip-адрес
мастера. И запуск осуществляется сначала мастера, а потом слэйвов. Таблицу сетевых адресов поддерживает мастер. 

Всех тонкостей я не вспомнил, сейчас все равно начну не с сетевого взаимодействия, а с одиночного мастера. 

### 20191104 12:56
Продолжаю, надеюсь - с ускорением...

Разделил хранилище на секцию и клиента. Секция действительно предоставляет хранение информации, клиент - интерфей к хранилищу. 
Не все пока разделено правильно, но я над этим буду работать. Сейчас система по-прежнему работает в режиме мастера и с одной 
(мастерской) секцией. Мастер должен быть еще и местом реализации алгоритма обработки. 

Сначала буду делать такой же клиент, располагаемый в ОЗУ как и мастерский. Потом, когда отлажусь, сделаю сетевой. 

Проверил "crazy" идею: заменить хеш Dictionary на SortedDictionary. Действительно все сработало, только результаты оказались 
заметно хуже. Граф формировался 245 сек. использовав 2.55 Гб ОЗУ. 

### 20191105 06:45
Пришла в голову идея как экономить память на таблице имен. За счет колиества проходов. Действительно, в первом проходе обрабатывать
не все коды, а только часть. Потом освободить память и сделать второй и т.д. проходы. Надо прикинуть. Есть для этого стенд таблицы
имен. 

Как ни странно, идея похоже не так глупа. На стенде я делаю следующее: 
1. Преобразую текстовый файл ридов в бинарный файл последовательности кодов слов, это требует 5-6 секунд.
2. Определяю, что буду работать с nsec секциями, в эксперименте установлено 8 секций
4. Делаю цикл по секциям, в каждом цикле сканирую бинарный файл и формирую словарь только для слов, младшие биты которых равны номеру
секции (каждый проход 0.8-0.9 сек. без формирования словаря, 1.9 сек. с построением словаря, проверка была для 8 и 16 секций).

Оперативная память захватывается словарем, используется около 250 Мб. ОЗУ, выглядит неплохо. Порожден временный файл объемом 315 Мб.
Предположим, эту схему можно будет применить для всей и большой задачи. Тогда, если величины не сильно "поплывут", задачу с 25 млн. 
ридов потребует 13 Гб. ОЗУ, что не сильно (всего в 1.5-2 раза) отличается от питерской программы. Время построения будет порядка 
20-40 минут, что также соответствует. Время обработки графа должно быть небольшим. 

Что я пока не сделал, это преобразование ридов, кодированных синтаксически в риды, кодированные номерами узлов. Это можно сделать 
двумя способами. Первый - изготовить "шаблон" кодированных ридов, а потом на каждом проходе и на каждой обработке слова, заменять
коды в шаблоне. Второй, в принципе аналогичный. Надо сохранять пары номер_слова-код_слова в файлах, а потом отдельным проходом,
открыв все такие файлы, произвести их слияние. Первый выглядит более простым, хотя получается, что добавится еще 5 секунд.
Дополнительный временный файл также будет увеличен до 16 Гб. 

### 20191106 06:39
Хочу попробовать разбить программу на две: одна будет вычислять граф, другая будет анализировать граф. Во второй программе не будет 
использоваться хеш-словарь. Начну с конца, как должна выглядеть структура данных, пригодная для анализа графа? Очевидно, что это
распределенный массив. Он должет быть распределен по свойствам индекса и, возможно, по полям - предполагаю, что выгодно будет 
отделить поле"имени", т.е. первичного кода узла. Первичным кодом назовем синтаксическое кодирование с упаковкой битов в какую-то 
структуру. 

Сейчас не будем заморачиваться по поводу того, чтобы отделять поля, первичное кодирование будет 64-х разрядным, вторичное пока 
32-х разрядным. Потом вторичное кодирование надо будет сделать 64-х разрядным, а первичное - k-байтным.

Итак, на узлах должен храниться респределенный бинарный образ массива узлов. Узел, это запись типа:
```
Node = prev, next: Integer, acode: Longinteger;
```
Технически, в узлах будут массивы этих узлов. Код узла: 
```
(номер_в_подмассиве * число_секций) + номер_секции
```

Но это - концовка. А начало - перевод ридов в бинарную форму. Это делается полностью на мастере. 500 млн. ридов для человека,
это около 300 Гб. диска. Прокачка таких данных по сети, например 1 Гбит, будет выполняться около часа. На то они и "большие данные".
Риды преобразую в бинарный файл формата [[CWord]], т.е. последовательность последовательностей слов синтаксического кодирования.
Это я могу сделать это быстро.

Сделал. Следующая фаза - кодирование ридов. В принципе, кодирование может быть совмещено с формированием графа, но пока лучше 
сделать независимо. Это преобразование выполняется пословным (новым) кодированием с сохранением структуры ридов. Кодирование
это не только пословное преобразование, но и формирование распределенного списка (массива) узлов. Надо сканировать риды и слова 
посылать разделам. Причем не все слова, а только слова для какой-то i-ой секции. В разделах надо накапливать словарь секции 
раздела и последовательность узлов. 

Двигаюсь в сторону разделения обработки на отдельные программы. Сделал Program3 - формироавние бинарных ридов. Начал работать над
Program4 - кодирование бинарных ридов.

Небольшое расхождение в результатах. Получилось узлов: 23770764, а было 23770815 (на 151 меньше...). Проверил, нет было все-таки как
сейчас, привожу "распечатку" результатов расчета еще не стертой программой:

Start DeBreinOrtho!
lines:495108 nodes: 23770764
Graph ok. Duration=58279
TotalMemory used: 1891759344
11135 297397 295203 23167029
follow: 0 not follow: 23462232
Build statistics ok. Duration=748
==== nchains: 377529  maxchain: 894
TGTCGCTGACCTATCCTGTAATCATTCTATAAGAAAATCCATTCCAAGAATATTTACTTAGATGTTATAATATTTATTATAACAAATTAAGGGATATTTATAAATTTGGGTATATCTCATAAGATACATATAAATATTAATATAATAATGCTGATTAATCTTAATATAACTACCATAAAGATATAATTTAACGCTTTTAATTTTTTTATCCACCGAATCCTTTACGACCATTATTAACTCATTCGTCATATTTTCTGTAATAAAGTAAGTTCCCCCAGGAACATTTTCTAGCAAACGATATATTAGACAACAATCTTCTGGAGGATGATAAAAATTTAGCTTTTTTAAATTCTTCCGTAATTTTTTTTCCTGGATTTTACGAGACCTACGATAAAATATACATTTTATCGGATTGCGAAATAACCTAACCATCTTCCTCGACTAACTAAGGATAGTTTCCTTATTTTAACAGTCATTAGAAATAAAGTTTATTACAAAATATATTAATTTTTTATAAAATTATTTCTTGTTAAAAATCATCATTTTTAATTGGAATCCTATTATAGACAATAAAATATCATTATTAGTAATAATTAAACGGCTCTAAATTTTAGTTCTTTGTCTTCAACTTAGAGGTCTAGTATAGGGTAGCCACTATGAAGGTGCTAATTCTAGTACTACTCGGGGTGGTTATCCTTCAGGCCGCACCTATACGTAAATTAGAAGATCTACTACCAACCCGTTATCCTCCTAAACATGAGCTGGTTTATTGGTGCACCTACGCAAATCAATGTGACTTTTGCTGGGAATGCGTCCACGGTATCTGCCGAAACAGGATTCAAGCAGATTGGCCAGTTATTCACCAAAATGACTGGATTATAAATTGCACGGTTTCCCGCTGGCTAAATAAATG
Extract chains ok. Duration=2171

### 20191108 09:12
Все же справился с проблемами. После исправлений и корректировок, получились следующие результаты по проходам обработки:
```
5.6 s - переобразование в бинарную форму
12.7 s - кодирование узлов (использует около 1.5 Гб ОЗУ)
1.5 s - создание графа
0.9 s - анализ цепочек
```
Итого: 20.7 секунды. Вполне неплохо для начала. 

Первый этап оптимизировать (пока) не нужно. Файл ридов локализуется на мастере, бинарный файл ридов пишется туде же. Предположительно,
время работы будет расти линейно. Т.е. если задача увеличится в 1000 раз, то этап будет работать часа полтора, два. Распараллелить его
довольно просто, но пока это не актуально.

Второй этап - самый ресурсоемкий, но я уже знаю схему решения этих проблем. Возможно, потребуется объединить действия второго и
третьего этапов, но пока попробую сделать их по отдельности. 

На втором этапе задача распределяется по хранилищам через распределение по частям (parts). соответственно, будет nparts частей,
каждая из которых отвественная за свой слой слов. Часть содержит слова своего слоя, выстроенные в список. Таким образом, данные
на этом этапе будут: массив кодированных ридов и распределенная система частей. Для каждого слова нужно выполнить 
```
code = GetSetNode(word); 
```
Причем с побочным эффектом в виде накапливания слов в списке слов. Система распределенная, поэтому часть должна быть оформлена как 
объект и добавляться к оператору
```
code = parts[...].GetSetNode(word); 
```
Где номер части вычисляется по значению word. Заметим, что "глобальной" или "сплошной" системы кодов не получится. Код должен 
содержать в себе номер секции и номер в списке секции. Окончательное решение:
```
nparts - степень двойки 1, 2, 4, 8, ...
nshift - количество битов в адресе parts, 0, 1, 2, 3, ... 

ipart = word & (nparts-1);
code = parts[ipart].GetSetNode(word); // code состоит из (nom << ishift) | ipart

ipart = code & (nparts-1);
word = parts[ipart].GetWord(code >> nshift);
```

Для начала, пусть будет одна секция. Внедрю решение. 

20191109 16:26
Сделал существенную переработку всего комплекса. Главное, я разбил программу на отдельные проходы, которые можно запускать как 
по-очереди, так и отдельно. Также я выделил отдельную структуру DeBruGraph, которая состоит из списков узлов, разбитых на части. 
Все это "богатство" заработало и готово к модификациям. А модификации потребуются. 

Буду оценивать характеристики задачи не только на тесте в 0.5 млн. ридов, но и на условных 25 млн. Видимо, это уже большие данные, но
посильные для одного компьютера со скромной (8-16 Гб.) ОЗУ. Первый этип не требует особых ресурсов, его можно оценивать линейно 
относительно размера задачи. Таким образом, по первому этпу никакой особой оперативной памяти не нужно, а производитльность 
оценивается в 5 минут для задачи в 25 млн. ридов. 

Следующий этап - первичное кодирование ридов. Видимо, это самый ресурсоемкий этап. И время тратится 13-14 сек. и ОЗУ используется
на создание таблицы имен. Надо замерить использование ОЗУ. Замерил. Используется ОЗУ на уровне 1.5 Гб. Надо внедрить две 
ОЗУ-сберегающие технологии. Одну - вводить данные в несколько проходов. Это делается легко и с малым дополнительным расходом времени.
Попробую. 

20191110 16:05
Я сейчас работаю над оптимизацией прохода первичного кодирования. Сначала пытаюсь реализовать многопроходную обработку. После
первых неуспешных попыток, начал разбираться и делать эксперименты. Вопрос не в самой многопроходной схеме, а в сборке результирующего 
файла с кодированными ридами. Есть несколько схем такой сборки. Я пошел по варианту, когда сборочный вариант в прходе используется 
и для записи и для чтения. В отдельном эксперименте было показано, что это действительно медленный способ. Более быстрый способ - 
работать с тремя потоковыми файлами. Это основной - бинарный поток ридов. Второй - выходной поток кодированных ридов, третий поток -
поток кодированных ридов, но взятый с предыдущего прохода. Тестовые результаты получились 23.5 sec на "стандартный" вариант 
(обрабатывается один файл) и 29 sec на работу с тремя файлами. 

Кстати, мне подумалось, что ожидаемое "отставание" моей программы от питерской может определяется характеристиками внешнего носителя.
Все же говорят, что твердтельный диск существенно быстрее.

Попробую сформировать подобное решение там, где это нужно. 

Дело движется. Уже могу запускать вторую стадию обработки (кодирование ридов) с несколькими слоями. Правда оказалось, что используется 
больше, чем четверть гигабайта...

Винмательно посмотрел, оказалось, что задача и 500 тыс. ридов укладывается в 0.5 Гб. Вроде бы... Это многовато, но допустим... 
Получается, что задача в 15 млн. ридов имеет шанс уложиться в 16 Гб. ОЗУ. Характеристики хуже, чем у питерцев, но уже близки. Зато
есть основания считать, что можно будет эффективно использовать кластерную архитектуру. И тогда 20-100 узлов хватит (?) для работы с 
человеческим геномом. 

### 20191111 08:21
Текущий расклад по временам обработки:
- перевод ридов в цепочечную и бинарную форму 5.6 сек
- кодирование бинарных ридов 33.7 сек
- создание графа 4.2 сек.
- анализ графа, извлечение цепочек 2.9 сек.

Всего где-то около 47 сек.

### 20191112 05:56
Теперь пора делать (в очередной раз) кластерную программу. Кажется, будет несколько проще. Надо сделать более эффективным следующий
цикл:
```
    for (int nom = 0; nom < nwords; nom++)
    {
        // Читаем, читаем, пишем
        UInt64 bword = br.ReadUInt64();
        int code = -4;
        if (lay > 0) code = binr.ReadInt32();
        if (((bword >> graph.nshift) & (ulong)(nlays-1)) == (ulong)lay) 
        {
            code = graph.GetSetNode(bword);
        }
        binw.Write(code);
    }
```
Эффективность нового решения видится в том, что действия GetSetNode и Write группируются и выполняются отложено. Насколько 
отложено? Максимум - до востребованности файла binw. То есть, до начала обработки следующего слоя. Теперь понятно где ставить
Flush(), который понадобится. Попробую сначала это преобразование сделать "в лоб": объявлю акцию, а потом ее исполню. 

Немножко "помыкался" и поошибался, но пришел к конструкции, которую не так просто изложить в виде текста. Суть конструкции в том,
что в программе выделяются участки, которые можно выполнять отложенно. Эти участки заменяются манипуляциями с активным буфером. При
этом, вместо выделенного фрагмента, остается оператор помещения в буфер команды, ее параметров и программы (ламбда выражения) действия.
Попробую...

Начинает получаться, но продолжается деградация производительности данного (второго) этапа. 
- прямое вычисление хеш-словаря 15.7 сек.
- вычисление хеш-словаря в 8 проходов 46.5 сек
- 8 проходов и группирование (выделение в группу) 61.9 сек.

Теперь надо расширить диапазон группирования, для этого, нужны новые команды. Делаю команды 1 - запись кода, 2 - вычисление и 
запись кода, 3 - запись числа ридов

### 20191113 06:45
Группирование, это только этап. Теперь надо собрать обращения к хранилищу, выполнить его массово и аккуратно доделать нужные операции.
Попробую это сделать. 

Сделал. Сделал и следующий этап - переход на несколько частей. Правда снова увеличилось общее время выполнения задачи. Теперь оно:
- 8 проходов, 2 части, обе на мастере 74.6 сек.

Проверил и на 4-х частях. Никаких видимых отличий. А насчет ухудшения времени, я ошибся. Это в режиме Debug 74 сек. А на самом деле:
- 8 проходов, 4 части, обе на мастере 64.7 сек.

Теперь - самое главное. Нужно сделать сетевой вариант части NodesPart и задействовать его в вычислениях. Начну с реализации массовой 
операции графоом и трансляции ее в секцию.

### 20191114 08:52
Медленно, но верно двигаюсь в сторону сетевого решения. Все подготовительные операции сделал, осталась сама IP-коммуникация.

Теперь вместо "нормальной" секции, надо сделать две вещи: сетевую секцию и коннектор. Исполнители будут клиентами. 
Сначала сделаю сервера. Путь ждет...

Сделаю клиента 
базе имеющегося решения. Делаю разветвление в Main и коннектю  

### 20191116 11:06
Вот я уже в Москве. Начинаю работать с того, что восстанавливаю работоспособность того, что было. Оказалось, что тестовые риды я не взял, 
пришлось решать эту проблему. Вспомнил как сделать точку доступа, сделал. На рабочем компьютере нашел файл, сжал его, попытался стандартным
образом (CntlC, CntlV) перекинуть его сюда - что-то не получилось. Попытался послать его по почте - также что-то не получилось. Воспользовался
гитом и гитхабом - получилось. Провел расчет, результаты по производительности получились приблизительно вдвое хуже, но все же получились. Времена
такие: 13.3 сек., 537 Мб, 140 сек, 11.6 сек, 8.6 сек.

При запуске была заметна недоделка в части управления параметрами задачи, в частности, именами файлов. Надо это изменить, а то и некрасиво и 
долго объяснять. Пока видится более удобным вносить параметры при запуске Main. Параметры задаются для мастера.

Как-то стройной "картины" не получается, займусь этим позже. А пока начну создавать сетевую конфигурацию. Для начала - примитивное общение мастера и
клиента. Мастер посылает однобайтовую команду, клиент ее получает и возвращает целое, заодно и распечатывает событие. 

### 20191117 11:53
Сделал минимальную коммуникацию, добавил Main3 как этап, который выполняется на мастере. Теперь нужно делать Main43. Пусть это будет Main44. Сделаю 
прототип, запущу прототип. 

### 20191118 07:58
Попробую сделать только одно хранилище с сетевым доступом к нему. То есть, вся задача - на мастере, включая все данные и почти все промежуточные 
результаты. Исключение будет только для списка узлов. Он будет создаваться и накапливаться на одном клиенте. Можно предположить, что список "там"
будет соответствовать тому, который получается в автономном варианте и следующие фазы можно будет сделать снова на мастере, подключив файл к следующим
этапам. ДЛя выполнения завершающих этапов в вычисления длинного контига, требуется наличие файлов creads.bin и nlist.bin. Оба создаются на этапе 
44, причем первый - прямо мастером, а второй надо будет сформировать сетевым образом. Как-то так...

Буду пробовать, причем имена файлов можно вставлять прямо в код. Итак, упростил ситуацию до одной секции (правда оставил 8 проходов). Только теперь
секцию надо сделать клиентской, а в мастере всесто секции использовать сетевой коннектор. 

### 20191119 16:11
Похоже, значительный (может огромный?) успех! Я сделал отладочную конфигурацию, в которой есть одна секция и он а сетевая. Выполнялись Main3 и Main44.
Сначала было довольно медленно, но это я ожидал - дело в мелких сетевых запросах. Просто буферизовал сетевые стримы и результаты получились довольно
симпатичные. То, что расчет был произведен правильный, я подствердил отдельным пропуском с измененной конфигурацией, когда я "досчитал" до самого 
длинного контига, который оказался тем же, что и ранее.

Теперь можно будет доделать. Не вижу препятствий. 

### 20191120 07:12
Уже почти дошел до конца. Теперь осталось совсем немного. Хотя именно этап анализа графа и выделения цепочек может оказаться 
"камнем преткновения". Выяснилась одна, думаю, что не очень существенная, деталь. Количество узлов оказалось ровно в 2 раза
больше, чем должно быть. Думаю, это устранить не сложно. Более существенным, является медленность текущего сетевого решения.
Пока я не могу дождаться конца обработки. Оценка времени - единицы часов. Может - десятки. Это многовато.

Сейчас поищу место дублирования узлов. Провел повторный расчет, дублирования не произошло. Наверное, где-то есть "дыра",
но сейчас ее искать не очень актуально. Актуальнее разработать более экономную схему поиска цепочек. Идея заключается в 
том, что поиск цепочек - процесс независимый. А еще, этот процесс можно разбить на поиск начальной точки и поиск цепочки.
Число цепочек в действующем примере довольно большое - более 300 тыс. И в то же время, число цепочек в 100 раз меньше 
числа узлов. Есть смысл поработать в этом направлении. Кроме того, можно сократить пересылки при выборке узлов за счет 
того, чтобы пересылать только ссылки.

Рассмотрим процесс поиска узлов как параллельный процесс. Первая фаза понятна: мы проходим все узлы, группируя их в набор
кодов узлов. Посылаем, получаем узлы. Некоторые узлы будут отбракованы сразу же, по характеристикам ссылок. 

Проявил терпение и ... посчиталось. Выделенная цепочка (вроде) правильная, было потрачено 4564 сек. - несколько более часа. 

"Старым" способом цепочки собираются 8.6 сек - в 500 раз быстрее!.. Теперь преобразую программу в два прохода, или может
даже 3 прохода. 

Отделил сбор начал цепочек от отслеживания цепочек. Теперь надо отделить этап заглядывания в предыдущий узел. 

1381 сек. - теперь это время вычисления последнего этапа Main62(). Секунд 25 теперь вычисляются начала цепочек. Буду доделывать 
ортогонализацию этого этапа. Идея в том, чтобы взять несколько цепочек и начать их наращивать. Если цепочка заканчивается, то
ее "утилизировать" и заменять на новую. Попробую обойтись имеющимися массовыми операциями. 

### 20191121 13:23
Кое-что доделал, справил несколько ошибок, теперь работает. Профиль исполнения для случая одного сетевого узла, отличного от
мастера: 13, 105, 28, 51 сек. - немного больше 3-х минут! Это на медленном копьютере. Теперь надо привести интерфейс с программой
в порядок и можно будет посылать Сергею Трошкову. 

Как задавать конфигурацию? Для каждой задачи надо указывать файл для работы (хранения части графа). Для мастера надо еще иметь 
директорию, в которой развертывается работа. Сейчас файлы мастера задаются в конфигураторе.  

### 20191123 14:14
Вернулся домой, переконфигурировался, сделал проспуск с одним клиентским исполнителем. Получилось: 
6 сек, 48 сек., 14 сек, 23.7 сек. Можно сказать, что вдвое быстрее. 

### 20191124 08:54
Вчера, по приезду, совсем не было сил работать. Сегодня собираюсь это сделать...

Пока лежал, подумал о текущих недоработках. Может их есть смысл устранить именно сейчас? Попробую набросать список.
1) Наверное, будет более удобным, если указывать рабочие директории прямо при запуске. Напр. через параметр -d директория.
2) В мастере стоит проверять, что все исполнители подключились. По крайней мере, по числу подключившихся. 
3) Сейчас никак не идентифицируются и не нумеруются исполнители, видимо нет потребности. Но в дальнешем может понадобиться 
останавливать и возобнавляь расчеты...
4) Пока не сделана "раскладка" потока команды получения узлов по кодам на секции. И объединение результата. Кстати, а как это 
место реализовано?
5) Параметр числа проходов "запрятан", а проходы неправильно названы слоями.
6) 

А теперь попробую простые элементы исправить. Указываю рабочую директорию. Это либо параметр -d диретория, либо поле directory 
в Options. Если не директория не задана параметром, то берется из Options. Причем тогда на каждой машине запуска рабочая директория 
должна располагаться в том же месте. 

Сделаю (восстановлю) возможность обработки без сети. Число частей 1, число клиентов 0. 
В режиме одной части на мастере, характеристики получаются: 6, 42 (532 Мб), 5.3, 6.7 сек. (Меньше минуты!)
В режиме одной части на клиенте получаются: 6 сек, 48 сек. (130 Кб), 14.5 сек, 23.1 сек. Можно сказать, что вдвое быстрее. 

### 20191125 07:01
При модернизации интерфейса и испытаниях, выявилось наличие ошибки: на одной секции обработка выполняется правильно, на двух -
неправильно. Надо искать. Наверное, это из-за недоделанности распределения запросов по секциям и слияния результатов. Плохо, что 
сейчас затруднительно производить преобразование программы "шаг за шагом", но что поделаешь... Есть несколько мест, где 
распределение по секциям надо выполнять. Первое - при формировании таблицы имен, когда выполняется массовая операция
```
IEnumerable<int> GetSetNodes(IEnumerable<UInt64> bwords);
```
Операция выполняется через следующие этапы: 1) распределение слов по секциям; 2) выполнение запросов к секциям, получение
результатов; 3) сборка результата в нужном порядке, порядок определяется сохраненным массивом слов.

Следующая массовая операция в прописи ссылок в узлы. 
```
    graph.SetNodePrev(code, codeprev);
    graph.SetNodeNext(codeprev, code);
```
Особенностью операции является отсутсвие возвращаемых результатов действий. Это позволяет обойтись стандартной буферизацией
BufferedStream использванной на сетевых стримах. Я просто пишу в нужную секцию, там это и буферизируется. 

Следующая массовая операция
```
	IEnumerable<CNode> GetNodes(IEnumerable<int> codes);
```
Она используется и на фазе формирования списка начальных узлов цепочек и на фазе прохождения цепочек. В принципе, она очень похожа
на GetSetNodes, вроде я ее даже реализовал, надо повнимательнее посмотреть код.

Сходил на работу. По поводу программы Сергей сказал, что она в сетевом режиме работает около 7 секунд. Неплохо... А я даже не 
оптимизировал используемые параметры. Наверное, резервы еще есть.  Теперь надо испытать программу на больших данных. Все бы ничего, 
но она рабботает некорректно для нескольких секций. Код я уже смотрел, "бяку" визуально не выявил, надо искать. Попробую выявить 
этап, на котором появляется ошибка. Для этого нужна программа слияния последовательностей CData. 

### 20191126 07:05
Снова углубился в размышления. Оказалось, все не так просто. Объединение списков clist достаточно просто перед последним этапом 62.
Это выявление цепочек и там не используются кодированные риды. А на предыдущих этапах кодированные риды используются и все не столь
очевидно. Хотя может и там можно... Просто в ридах имеются измеренные цепочки кодов, выраженные фактически в виде пар номер секции-
номер узла в секции. Изменение этой нумерации заставляет переносить изменения в риды или как-то по-другому. Поскольку я ищу 
конкретную ошибку, можно попробовать все же слить, может ошибка именно в 62-м этапе. 

Попробовал, написал программу, исправил пару ошибок и... Убедился, что ошибка не в 62-м. Как-то теперь надо бы проверить 51-й и
44-й. Посмотрю код. 

О чудо, посмотрел, в одном месте сдела более мнемоничные идентификаторы и нашел ошибку. Исправил и программа заработала! 

Сейчас займусь доделыванием программы до состояния, когда можно будет ее испытать. Делаю следующее: 
1) В конфигураторе задается только то, что должно задаваться, остальное вычисляется. При запуске у мастера задается число
клиентов, а у клиентов - IP мастера
2) Вытащить в опции все параметры расчетов, включая число проходов.

Вроде все. Начинаю. 





